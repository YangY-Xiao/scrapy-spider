#encoding=utf-8

import scrapy
import os
import sys
import urllib2
import random
import time

class PacketStormSpider(scrapy.Spider):
    #name:identifies the Spider. It must be unique within a project,
    name = "packetstorms"

    def start_requests(self):
        url_main = 'https://packetstormsecurity.com/files/tags/exploit/page1/'
        yield scrapy.Request(url=url_main,callback=self.parse)


    def parse(self,response):
        '''
        page = response.url.split("/")[-2]
        filename = 'packetstorm-%s.html' % page
        with open(filename, 'wb') as f:
            f.write(response.body)
        self.log('Saved file %s' % filename)
        '''

        info = []

        for item in response.xpath("//div[@id='m']/dl"):
            info_detail = {}

            #title
            title = item.xpath("dt/a/text()").extract_first()
            info_detail['title'] = title

            #posted datetime
            datetime = item.xpath("dd[@class='datetime']/a/text()").extract_first()
            info_detail['datetime'] = datetime
            
            #refers: Authored by | Site
            refers = item.xpath("dd[@class='refer']/a/text()").extract()
            authors = item.xpath("dd[@class='refer']/a[@class='person']/text()").extract()
            sites = list(set(refers) - set(authors))
            info_detail['authors'] = authors
            info_detail['sites'] = sites

            #detail:list
            detail = item.xpath("dd[@class='detail']/p/text()").extract()
            info_detail['detail'] = detail

            #tags:list
            tags = item.xpath("dd[@class='tags']/a/text()").extract()
            info_detail['tags'] = tags

            #systems:list
            systems = item.xpath("dd[@class='os']/a/text()").extract()
            info_detail['systems'] = systems

            #advisories:list
            advisories = item.xpath("dd[@class='cve']/a/text()").extract()
            info_detail['advisories'] = advisories

            #md5:list
            md5 = item.xpath("dd[@class='md5']/code/text()").extract()
            info_detail['md5'] = md5

            #download_site
            download_site = item.xpath("dd[@class='act-links']/a/@href").extract_first()
            if download_site is not None:
                download_site = response.urljoin(download_site)
                #urllib open download
                response_download = urllib2.urlopen(download_site)
                filename = download_site.split("/")[-1]
                with open("download/"+filename,"wb") as f:
                    f.write(response_download.read())
                print "---Download is completed---"
                
            print info_detail
            print '-----------------------------------'
            
            time.sleep(5+random.randint(5,10))
       
        next_page = response.xpath("//div[@id='nv']/a[text()='Next']/@href").extract_first()
        if next_page is not None:
            next_page = response.urljoin(next_page)
            yield scrapy.Request(next_page,self.parse)
        
